{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:06:10.176365Z",
     "start_time": "2024-08-13T12:06:10.173847Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Chempy.parameter import ModelParameters\n",
    "\n",
    "import sbi.utils as utils\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "import time as t\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da844fae726a32e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ------ Load & prepare the data ------\n",
    "\n",
    "# --- Load in training data ---\n",
    "path_training = '../ChempyMulti/tutorial_data/TNG_Training_Data.npz'\n",
    "training_data = np.load(path_training, mmap_mode='r')\n",
    "\n",
    "elements = training_data['elements']\n",
    "train_x = training_data['params']\n",
    "train_y = training_data['abundances']\n",
    "\n",
    "\n",
    "# ---  Load in the validation data ---\n",
    "path_test = '../ChempyMulti/tutorial_data/TNG_Test_Data.npz'\n",
    "val_data = np.load(path_test, mmap_mode='r')\n",
    "\n",
    "val_x = val_data['params']\n",
    "val_y = val_data['abundances']\n",
    "\n",
    "\n",
    "# --- Clean the data ---\n",
    "# Chempy sometimes returns zeros or infinite values, which need to removed\n",
    "def clean_data(x, y):\n",
    "    # Remove all zeros from the training data\n",
    "    index = np.where((y == 0).all(axis=1))[0]\n",
    "    x = np.delete(x, index, axis=0)\n",
    "    y = np.delete(y, index, axis=0)\n",
    "\n",
    "    # Remove all infinite values from the training data\n",
    "    index = np.where(np.isfinite(y).all(axis=1))[0]\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_x, train_y = clean_data(train_x, train_y)\n",
    "val_x, val_y     = clean_data(val_x, val_y)\n",
    "\n",
    "# convert to torch tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "val_x = torch.tensor(val_x, dtype=torch.float32)\n",
    "val_y = torch.tensor(val_y, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T10:58:26.680424Z",
     "start_time": "2024-08-13T10:58:26.597006Z"
    }
   },
   "id": "197d45fe9c2c3049"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "a = ModelParameters()\n",
    "labels = [a.to_optimize[i] for i in range(len(a.to_optimize))] + ['time']\n",
    "priors = torch.tensor([[a.priors[opt][0], a.priors[opt][1]] for opt in a.to_optimize])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T11:50:39.360458Z",
     "start_time": "2024-08-13T11:50:39.341219Z"
    }
   },
   "id": "bbb2b36f70fe6637"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the NN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec047e04bed15c02"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps\n"
     ]
    },
    {
     "data": {
      "text/plain": "Model_Torch(\n  (l1): Linear(in_features=6, out_features=100, bias=True)\n  (l2): Linear(in_features=100, out_features=40, bias=True)\n  (l3): Linear(in_features=40, out_features=9, bias=True)\n)"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    print(\"using mps\")\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class Model_Torch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Torch, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(train_x.shape[1], 100)\n",
    "        self.l2 = torch.nn.Linear(100, 40)\n",
    "        self.l3 = torch.nn.Linear(40, train_y.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the input\n",
    "        \"\"\"x1 = (abs(x[:,0:-1]-priors[:,0]))/(priors[:,1]*10)\n",
    "        x2 = x[:,-1]/torch.tensor([13.8])\n",
    "        x = torch.cat((x1, x2.reshape(-1,1)),1)\"\"\"\n",
    "        \n",
    "        x = torch.tanh(self.l1(x))\n",
    "        x = torch.tanh(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "model = Model_Torch()\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:00:02.057322Z",
     "start_time": "2024-08-13T12:00:02.037986Z"
    }
   },
   "id": "771bd6cef2b791b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1bf0bfd03c2313"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 in 2.5s, Loss: 0.000933 | Val Loss: 0.001334\n",
      "Epoch 2/15 in 2.6s, Loss: 0.00058 | Val Loss: 0.000586\n",
      "Epoch 3/15 in 2.6s, Loss: 0.000372 | Val Loss: 0.000482\n",
      "Epoch 4/15 in 2.7s, Loss: 0.000264 | Val Loss: 0.000417\n",
      "Epoch 5/15 in 2.6s, Loss: 0.000224 | Val Loss: 0.000355\n",
      "Epoch 6/15 in 2.6s, Loss: 0.000206 | Val Loss: 0.000297\n",
      "Epoch 7/15 in 2.6s, Loss: 0.000177 | Val Loss: 0.000262\n",
      "Epoch 8/15 in 2.6s, Loss: 0.000153 | Val Loss: 0.000242\n",
      "Epoch 9/15 in 2.7s, Loss: 0.000137 | Val Loss: 0.000228\n",
      "Epoch 10/15 in 2.7s, Loss: 0.000125 | Val Loss: 0.000217\n",
      "Epoch 11/15 in 2.6s, Loss: 0.000117 | Val Loss: 0.000209\n",
      "Epoch 12/15 in 2.6s, Loss: 0.000112 | Val Loss: 0.000201\n",
      "Epoch 13/15 in 2.6s, Loss: 0.000106 | Val Loss: 0.000195\n",
      "Epoch 14/15 in 2.6s, Loss: 0.000102 | Val Loss: 0.00019\n",
      "Epoch 15/15 in 2.7s, Loss: 9.9e-05 | Val Loss: 0.000185\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# shuffle the data\n",
    "index = np.arange(train_x.shape[0])\n",
    "np.random.shuffle(index)\n",
    "train_x = train_x[index]\n",
    "train_y = train_y[index]\n",
    "\n",
    "# --- Train the neural network ---\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    start_epoch = t.time()\n",
    "    for i in range(0, train_x.shape[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get the batch\n",
    "        x_batch = train_x[i:i+batch_size].detach().clone().to(device).requires_grad_(True)\n",
    "        y_batch = train_y[i:i+batch_size].detach().clone().to(device).requires_grad_(True)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation loss\n",
    "    y_pred = model(val_x)\n",
    "    val_loss = loss_fn(y_pred, val_y)\n",
    "        \n",
    "    end_epoch = t.time()\n",
    "    epoch_time = end_epoch - start_epoch\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs} in {round(epoch_time,1)}s, Loss: {round(loss.item(),6)} | Val Loss: {round(val_loss.item(),6)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:00:49.189118Z",
     "start_time": "2024-08-13T12:00:09.626849Z"
    }
   },
   "id": "cbc674932e28392b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Save the model ---\n",
    "torch.save(model.state_dict(), 'data/pytorch_state_dict.pt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "715398975ac1e67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train SBI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90e9435d8354a086"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Load the model ---\n",
    "model = Model_Torch()\n",
    "model.load_state_dict(torch.load('data/pytorch_state_dict.pt'))\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6acdfd8f78add5f1"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "combined_priors = utils.MultipleIndependent(\n",
    "    [Normal(p[0]*torch.ones(1), p[1]*torch.ones(1)) for p in priors] +\n",
    "    [Uniform(torch.tensor([2.0]), torch.tensor([12.8]))],\n",
    "    validate_args=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:06:36.391052Z",
     "start_time": "2024-08-13T12:06:36.376703Z"
    }
   },
   "id": "e21d67937e4b8172"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "def simulator_NN_torch(params):\n",
    "    pred = model(params)\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:06:37.024454Z",
     "start_time": "2024-08-13T12:06:37.015126Z"
    }
   },
   "id": "45cefd390c8f9164"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [9]], which is output 0 of StdBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[154], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m start \u001B[38;5;241m=\u001B[39m t\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      6\u001B[0m theta, x \u001B[38;5;241m=\u001B[39m simulate_for_sbi(simulator, proposal\u001B[38;5;241m=\u001B[39mprior, num_simulations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m density_estimator \u001B[38;5;241m=\u001B[39m inference\u001B[38;5;241m.\u001B[39mappend_simulations(theta, x)\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      8\u001B[0m posterior \u001B[38;5;241m=\u001B[39m inference\u001B[38;5;241m.\u001B[39mbuild_posterior(density_estimator)\n\u001B[1;32m     10\u001B[0m end \u001B[38;5;241m=\u001B[39m t\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[0;32m~/miniconda3/envs/master_chempy_multi/lib/python3.12/site-packages/sbi/inference/snpe/snpe_c.py:180\u001B[0m, in \u001B[0;36mSNPE_C.train\u001B[0;34m(self, num_atoms, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, use_combined_loss, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_non_atomic_loss:\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;66;03m# Take care of z-scoring, pre-compute and store prior terms.\u001B[39;00m\n\u001B[1;32m    178\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_state_for_mog_proposal()\n\u001B[0;32m--> 180\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/master_chempy_multi/lib/python3.12/site-packages/sbi/inference/snpe/snpe_base.py:367\u001B[0m, in \u001B[0;36mPosteriorEstimator.train\u001B[0;34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001B[0m\n\u001B[1;32m    364\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(train_losses)\n\u001B[1;32m    365\u001B[0m train_log_probs_sum \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m train_losses\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m--> 367\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    368\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m clip_max_norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    369\u001B[0m     clip_grad_norm_(\n\u001B[1;32m    370\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_neural_net\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39mclip_max_norm\n\u001B[1;32m    371\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/master_chempy_multi/lib/python3.12/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    526\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    527\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/master_chempy_multi/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m _engine_run_backward(\n\u001B[1;32m    268\u001B[0m     tensors,\n\u001B[1;32m    269\u001B[0m     grad_tensors_,\n\u001B[1;32m    270\u001B[0m     retain_graph,\n\u001B[1;32m    271\u001B[0m     create_graph,\n\u001B[1;32m    272\u001B[0m     inputs,\n\u001B[1;32m    273\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    275\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/master_chempy_multi/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    746\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [9]], which is output 0 of StdBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "simulator, prior = prepare_for_sbi(simulator_NN_torch, combined_priors)\n",
    "inference = SNPE(prior=prior)\n",
    "\n",
    "start = t.time()\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=100)\n",
    "density_estimator = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "end = t.time()\n",
    "comp_time = end - start\n",
    "print(f'Time taken to train the posterior with {len(train_y)} samples: '\n",
    "      f'{np.floor(comp_time/60).astype(\"int\")}min {np.floor(comp_time%60).astype(\"int\")}s')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:13:10.053559Z",
     "start_time": "2024-08-13T12:13:09.976788Z"
    }
   },
   "id": "24a9576707985533"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff1b1167e294230c"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:12:18.974979Z",
     "start_time": "2024-08-13T12:12:18.961478Z"
    }
   },
   "id": "30b6b40bf95b15b4"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-2.3841e+00, -3.2961e+00, -2.2128e-01,  5.3151e-01,  4.1123e-01,\n          8.0533e+00],\n        [-2.6094e+00, -2.9897e+00, -1.8209e-01,  5.8454e-01,  5.1260e-01,\n          1.1587e+01],\n        [-2.1794e+00, -3.0984e+00,  3.1821e-02,  5.8867e-01,  4.6967e-01,\n          4.7040e+00],\n        [-2.0311e+00, -2.4871e+00, -5.4967e-02,  4.1474e-01,  5.0354e-01,\n          1.0812e+01],\n        [-2.6379e+00, -2.9906e+00, -3.7574e-01,  7.3419e-01,  4.7525e-01,\n          2.2424e+00],\n        [-2.1731e+00, -2.8508e+00, -4.8300e-01,  5.3187e-01,  5.7189e-01,\n          5.0941e+00],\n        [-2.7868e+00, -2.4215e+00,  9.8619e-02,  3.8069e-01,  4.5382e-01,\n          9.9786e+00],\n        [-2.3006e+00, -3.1132e+00, -6.0857e-01,  4.1632e-01,  4.9090e-01,\n          5.9860e+00],\n        [-2.3414e+00, -3.2068e+00, -2.6241e-01,  5.9485e-01,  4.9363e-01,\n          7.1465e+00],\n        [-2.0368e+00, -2.7116e+00, -1.7991e-01,  5.5132e-01,  5.1690e-01,\n          4.8675e+00],\n        [-2.2917e+00, -2.8780e+00, -1.5314e-01,  4.7597e-01,  2.9847e-01,\n          1.2569e+01],\n        [-2.6490e+00, -3.5406e+00, -3.9227e-01,  5.4783e-01,  5.7294e-01,\n          3.4483e+00],\n        [-2.5728e+00, -2.5941e+00, -3.3126e-02,  4.6644e-01,  3.7457e-01,\n          4.1968e+00],\n        [-2.4053e+00, -2.7125e+00,  1.0828e-01,  5.3587e-01,  5.5676e-01,\n          3.6619e+00],\n        [-1.6806e+00, -3.0701e+00, -3.5413e-01,  5.7825e-01,  5.7543e-01,\n          4.4364e+00],\n        [-2.4337e+00, -2.7527e+00,  1.6486e-01,  6.9108e-01,  5.0219e-01,\n          8.0259e+00],\n        [-2.6162e+00, -2.9142e+00, -1.7586e-01,  5.2364e-01,  3.7266e-01,\n          6.1254e+00],\n        [-2.5766e+00, -3.2291e+00,  2.3080e-01,  6.1042e-01,  4.9777e-01,\n          1.0745e+01],\n        [-2.3149e+00, -3.0964e+00, -5.4619e-01,  4.7864e-01,  4.1661e-01,\n          1.1276e+01],\n        [-2.2319e+00, -2.6632e+00, -3.2540e-01,  5.5055e-01,  4.2187e-01,\n          8.1886e+00],\n        [-2.2029e+00, -2.5838e+00, -3.2755e-01,  4.2083e-01,  6.4104e-01,\n          6.1778e+00],\n        [-2.4512e+00, -2.8953e+00, -2.4051e-02,  5.9371e-01,  4.5241e-01,\n          1.1789e+01],\n        [-2.0036e+00, -2.8387e+00, -6.3716e-01,  6.1309e-01,  6.3055e-01,\n          7.4918e+00],\n        [-2.2702e+00, -2.2898e+00,  1.7503e-02,  4.8392e-01,  4.5335e-01,\n          1.2114e+01],\n        [-2.1891e+00, -2.9459e+00, -8.0564e-02,  5.5810e-01,  5.1476e-01,\n          7.5892e+00],\n        [-2.3231e+00, -2.6318e+00, -7.5740e-02,  6.1331e-01,  4.2505e-01,\n          1.2369e+01],\n        [-1.8408e+00, -3.1354e+00, -1.1930e+00,  5.3264e-01,  5.9118e-01,\n          5.5781e+00],\n        [-2.3760e+00, -2.3563e+00, -7.3920e-01,  4.0247e-01,  5.3105e-01,\n          2.8398e+00],\n        [-2.7048e+00, -3.0360e+00,  1.8703e-01,  6.1226e-01,  3.2373e-01,\n          6.8662e+00],\n        [-2.3945e+00, -3.0255e+00,  1.7713e-01,  4.8128e-01,  6.6006e-01,\n          1.0243e+01],\n        [-2.5206e+00, -3.2956e+00, -1.0453e-01,  4.0900e-01,  5.8614e-01,\n          1.1271e+01],\n        [-2.3349e+00, -2.7557e+00, -4.1192e-01,  6.1660e-01,  5.6351e-01,\n          7.6566e+00],\n        [-1.9975e+00, -3.1750e+00, -2.8969e-01,  5.8186e-01,  6.2936e-01,\n          1.0916e+01],\n        [-2.2819e+00, -2.7631e+00, -1.2596e-01,  2.9932e-01,  7.4118e-01,\n          5.2715e+00],\n        [-2.0778e+00, -2.8104e+00, -6.9496e-01,  5.4929e-01,  4.8561e-01,\n          3.0504e+00],\n        [-2.2158e+00, -3.2130e+00, -3.7325e-01,  4.9791e-01,  4.9325e-01,\n          2.1885e+00],\n        [-2.2762e+00, -3.0661e+00, -1.1607e-01,  9.1308e-01,  5.9833e-01,\n          4.3751e+00],\n        [-2.1601e+00, -2.4654e+00, -7.2541e-01,  7.0165e-01,  4.3389e-01,\n          3.2852e+00],\n        [-2.4707e+00, -3.3029e+00, -3.5623e-01,  5.7229e-01,  5.7591e-01,\n          7.6795e+00],\n        [-2.0625e+00, -2.9472e+00,  3.3817e-01,  4.1816e-01,  4.8375e-01,\n          8.5253e+00],\n        [-2.6145e+00, -2.6748e+00, -3.7709e-01,  6.2098e-01,  5.3167e-01,\n          1.2539e+01],\n        [-2.3993e+00, -2.7650e+00, -2.4025e-02,  5.0381e-01,  6.5882e-01,\n          1.0560e+01],\n        [-2.8433e+00, -2.7985e+00, -5.6908e-01,  5.0291e-01,  4.4513e-01,\n          1.1595e+01],\n        [-2.2320e+00, -2.8477e+00,  3.9103e-01,  6.6690e-01,  5.8299e-01,\n          4.5739e+00],\n        [-2.4432e+00, -3.2122e+00, -6.8135e-01,  5.8481e-01,  4.7767e-01,\n          4.0173e+00],\n        [-2.0807e+00, -2.4138e+00, -3.4892e-01,  5.5566e-01,  5.3378e-01,\n          5.6269e+00],\n        [-2.2438e+00, -3.2586e+00,  3.2797e-01,  5.7860e-01,  7.4089e-01,\n          6.5205e+00],\n        [-2.3416e+00, -3.4895e+00, -6.1996e-01,  5.0377e-01,  4.5628e-01,\n          1.0622e+01],\n        [-2.4781e+00, -2.7330e+00, -4.7613e-01,  3.5426e-01,  2.3435e-01,\n          1.2673e+01],\n        [-2.2893e+00, -2.7242e+00, -2.3556e-01,  4.2613e-01,  3.4121e-01,\n          9.0029e+00],\n        [-2.8090e+00, -2.6553e+00, -4.2307e-02,  7.2008e-01,  4.6245e-01,\n          3.5340e+00],\n        [-2.2514e+00, -2.3617e+00, -1.3186e-01,  5.0483e-01,  4.9441e-01,\n          1.0197e+01],\n        [-2.8934e+00, -3.5706e+00,  5.2300e-02,  5.8375e-01,  4.5388e-01,\n          1.2619e+01],\n        [-2.5544e+00, -3.1144e+00, -4.2181e-01,  3.5855e-01,  5.2363e-01,\n          1.1689e+01],\n        [-3.0968e+00, -2.9541e+00,  2.2095e-01,  4.3401e-01,  5.8025e-01,\n          1.1939e+01],\n        [-2.7065e+00, -3.0543e+00, -8.6268e-01,  5.7591e-01,  4.2613e-01,\n          6.8616e+00],\n        [-2.2107e+00, -2.7589e+00, -3.7894e-01,  5.2648e-01,  4.6088e-01,\n          1.2328e+01],\n        [-2.3531e+00, -2.7641e+00, -2.6926e-02,  6.8195e-01,  5.9958e-01,\n          2.1815e+00],\n        [-2.2464e+00, -2.5945e+00, -1.9738e-01,  5.3980e-01,  4.4248e-01,\n          6.9970e+00],\n        [-2.0387e+00, -2.5886e+00, -3.1436e-02,  4.5721e-01,  5.2065e-01,\n          1.0633e+01],\n        [-2.1985e+00, -3.3140e+00, -6.1969e-01,  3.9727e-01,  5.4672e-01,\n          1.1374e+01],\n        [-2.3331e+00, -2.7576e+00,  1.0094e-01,  4.0597e-01,  3.2893e-01,\n          1.0356e+01],\n        [-2.0485e+00, -3.0460e+00, -4.2471e-01,  6.5174e-01,  4.9510e-01,\n          4.9136e+00],\n        [-1.9169e+00, -2.7408e+00,  3.8554e-03,  4.3831e-01,  5.9325e-01,\n          2.9049e+00],\n        [-3.2373e+00, -2.8044e+00,  5.4198e-02,  5.3100e-01,  4.1507e-01,\n          1.1705e+01],\n        [-2.2277e+00, -3.2687e+00,  1.2766e-01,  5.1153e-01,  4.7788e-01,\n          2.7905e+00],\n        [-2.4682e+00, -3.0377e+00, -3.6600e-01,  9.0048e-01,  5.9053e-01,\n          1.0505e+01],\n        [-2.3642e+00, -2.6263e+00, -2.6201e-01,  4.6271e-01,  4.9119e-01,\n          1.2244e+01],\n        [-2.8274e+00, -3.2533e+00, -2.6017e-01,  6.7720e-01,  6.6363e-01,\n          1.1957e+01],\n        [-1.6184e+00, -3.3339e+00,  1.3686e-02,  5.8305e-01,  5.7743e-01,\n          7.0320e+00],\n        [-2.3237e+00, -2.6531e+00, -6.1292e-01,  5.9514e-01,  5.5617e-01,\n          7.4916e+00],\n        [-2.1933e+00, -3.3586e+00, -4.2147e-01,  4.8875e-01,  6.0779e-01,\n          7.7782e+00],\n        [-2.0263e+00, -2.3118e+00, -5.6074e-01,  5.4834e-01,  6.7022e-01,\n          4.0909e+00],\n        [-2.0241e+00, -3.2288e+00, -1.3772e-01,  4.6009e-01,  4.6493e-01,\n          7.1113e+00],\n        [-2.5651e+00, -3.2469e+00, -4.6563e-01,  3.8783e-01,  4.8085e-01,\n          2.9285e+00],\n        [-2.6803e+00, -3.6347e+00, -3.7790e-01,  4.7826e-01,  3.6769e-01,\n          1.2074e+01],\n        [-2.5107e+00, -2.9456e+00, -1.9518e-01,  6.2363e-01,  4.2147e-01,\n          6.6028e+00],\n        [-2.4141e+00, -3.1719e+00, -7.6424e-01,  3.5904e-01,  4.5565e-01,\n          8.5060e+00],\n        [-2.2964e+00, -2.8456e+00, -5.4755e-01,  6.7612e-01,  3.4925e-01,\n          1.1423e+01],\n        [-2.6004e+00, -3.3039e+00, -7.8821e-01,  5.6930e-01,  4.5947e-01,\n          8.8337e+00],\n        [-2.7217e+00, -2.7725e+00, -5.4944e-01,  3.0638e-01,  5.5482e-01,\n          3.6750e+00],\n        [-2.6156e+00, -3.2166e+00, -1.9864e-01,  3.8628e-01,  4.5672e-01,\n          1.0009e+01],\n        [-2.1470e+00, -2.6229e+00, -7.9858e-01,  5.3130e-01,  4.5559e-01,\n          8.4696e+00],\n        [-2.1989e+00, -2.8301e+00, -9.5517e-02,  6.1057e-01,  6.2933e-01,\n          5.4140e+00],\n        [-2.4301e+00, -3.2347e+00, -6.2947e-01,  4.5568e-01,  6.0270e-01,\n          4.5009e+00],\n        [-2.2865e+00, -2.9198e+00,  3.2747e-02,  7.0509e-01,  5.9820e-01,\n          1.2342e+01],\n        [-2.5909e+00, -2.9406e+00, -3.7822e-01,  3.8869e-01,  5.9675e-01,\n          9.4717e+00],\n        [-2.3801e+00, -3.0627e+00,  1.7486e-02,  6.2397e-01,  3.2157e-01,\n          8.8897e+00],\n        [-2.0097e+00, -2.7303e+00, -2.5927e-01,  5.7904e-01,  5.6565e-01,\n          1.1540e+01],\n        [-2.6470e+00, -3.0806e+00, -4.5065e-01,  6.0548e-01,  5.9304e-01,\n          1.0658e+01],\n        [-2.4101e+00, -2.5287e+00, -4.9469e-01,  6.0465e-01,  5.5001e-01,\n          6.6477e+00],\n        [-2.8602e+00, -2.8837e+00,  3.9625e-02,  5.4634e-01,  4.7596e-01,\n          1.0863e+01],\n        [-2.4343e+00, -2.7797e+00, -4.8919e-01,  6.1273e-01,  5.0487e-01,\n          7.1865e+00],\n        [-1.9131e+00, -3.0607e+00, -1.5283e-01,  7.2547e-01,  5.1700e-01,\n          1.1370e+01],\n        [-1.9009e+00, -2.9825e+00, -9.5001e-02,  5.6369e-01,  5.5547e-01,\n          3.2102e+00],\n        [-2.5211e+00, -2.6936e+00, -2.4467e-01,  4.7469e-01,  5.3167e-01,\n          1.2021e+01],\n        [-2.0374e+00, -2.6228e+00, -1.4518e-01,  5.7836e-01,  3.4195e-01,\n          7.6075e+00],\n        [-2.3174e+00, -2.9352e+00, -7.6409e-01,  6.4136e-01,  6.4018e-01,\n          1.1096e+01],\n        [-2.1166e+00, -2.9433e+00, -2.2919e-02,  3.6789e-01,  4.7712e-01,\n          1.0800e+01],\n        [-2.3248e+00, -2.6664e+00, -6.0189e-01,  5.2092e-01,  3.5644e-01,\n          8.1372e+00]])"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T12:12:31.353206Z",
     "start_time": "2024-08-13T12:12:31.337493Z"
    }
   },
   "id": "8f26a1296f2b4f68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
