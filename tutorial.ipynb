{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "from Chempy.parameter import ModelParameters\n",
    "\n",
    "import sbi.utils as utils\n",
    "from sbi.utils.user_input_checks import check_sbi_inputs, process_prior, process_simulator\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "import time as t\n",
    "from plot_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plots/sbi2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elements to track\n",
    "labels_out = ['C', 'Fe', 'H', 'He', 'Mg', 'N', 'Ne', 'O', 'Si']\n",
    "\n",
    "# Input parameters\n",
    "labels_in = ['high_mass_slope', 'log10_N_0', 'log10_starformation_efficiency', 'log10_sfr_scale', 'outflow_feedback_fraction', 'time']\n",
    "priors = torch.tensor([[-2.3000,  0.3000],\n",
    "                       [-2.8900,  0.3000],\n",
    "                       [-0.3000,  0.3000],\n",
    "                       [ 0.5500,  0.1000],\n",
    "                       [ 0.5000,  0.1000]])\n",
    "\n",
    "combined_priors = utils.MultipleIndependent(\n",
    "    [Normal(p[0]*torch.ones(1), p[1]*torch.ones(1)) for p in priors] +\n",
    "    [Uniform(torch.tensor([2.0]), torch.tensor([12.8]))],\n",
    "    validate_args=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Torch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Torch, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(len(labels_in), 100)\n",
    "        self.l2 = torch.nn.Linear(100, 40)\n",
    "        self.l3 = torch.nn.Linear(40, len(labels_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.l1(x))\n",
    "        x = torch.tanh(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "model = Model_Torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the weights ---\n",
    "model.load_state_dict(torch.load('data/pytorch_state_dict.pt'))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Set up the simulator -----\n",
    "def simulator(params):\n",
    "    y = model(params)\n",
    "    y = y.detach().numpy()\n",
    "\n",
    "    # Remove H from data, because it is just used for normalization (output with index 2)\n",
    "    y = np.delete(y, 2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior, num_parameters, prior_returns_numpy = process_prior(combined_priors)\n",
    "simulator = process_simulator(simulator, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulator, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulating data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9b051607db49e98aa26dad1d6d5d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genereted 10000 samples\n",
      "\n",
      "Training the posterior...\n",
      " Neural network successfully converged after 171 epochs.\n",
      "Time taken to train the posterior with 10000 samples: 10min 10s\n"
     ]
    }
   ],
   "source": [
    "# ----- Train the SBI -----\n",
    "inference = NPE(prior=prior, show_progress_bars=True)\n",
    "\n",
    "start = t.time()\n",
    "\n",
    "# --- simulate the data ---\n",
    "print()\n",
    "print(\"Simulating data...\")\n",
    "theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=10000)\n",
    "print(f\"Genereted {len(theta)} samples\")\n",
    "\n",
    "# --- add noise ---\n",
    "pc_ab = 5 # percentage error in abundance\n",
    "\n",
    "x_err = np.ones_like(x)*float(pc_ab)/100.\n",
    "x = norm.rvs(loc=x,scale=x_err)\n",
    "x = torch.tensor(x).float()\n",
    "\n",
    "# --- train ---\n",
    "print()\n",
    "print(\"Training the posterior...\")\n",
    "density_estimator = inference.append_simulations(theta, x).train()\n",
    "\n",
    "# --- build the posterior ---\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "end = t.time()\n",
    "comp_time = end - start\n",
    "\n",
    "print()\n",
    "print(f'Time taken to train the posterior with {len(theta)} samples: '\n",
    "      f'{np.floor(comp_time/60).astype(\"int\")}min {np.floor(comp_time%60).astype(\"int\")}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_GP = utils.MultipleIndependent(\n",
    "    [Normal(p[0]*torch.ones(1), p[1]*torch.ones(1)) for p in priors[2:]] +\n",
    "    [Uniform(torch.tensor([2.0]), torch.tensor([12.8]))],\n",
    "    validate_args=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_stars = 1000\n",
    "simulations = 1000\n",
    "\n",
    "stars = local_GP.sample((N_stars,))\n",
    "global_params = torch.tensor([[-2.3, -2.89]])\n",
    "\n",
    "stars = torch.cat((global_params.repeat(N_stars, 1), stars), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Simulate abundances -----\n",
    "start = t.time()\n",
    "abundances = model(stars)  \n",
    "# Remove H from data, because it is just used for normalization (output with index 2)\n",
    "abundances = torch.cat([abundances[:,0:2], abundances[:,3:]], axis=1)\n",
    "end = t.time()\n",
    "print(f'Time to create data for {N_stars} stars: {end-start:.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(true_abundances):\n",
    "    # Define observational erorrs\n",
    "    pc_ab = 5 # percentage error in abundance\n",
    "\n",
    "    # Jitter true abundances and birth-times by these errors to create mock observational values.\n",
    "    obs_ab_errors = np.ones_like(true_abundances)*float(pc_ab)/100.\n",
    "    obs_abundances = norm.rvs(loc=true_abundances,scale=obs_ab_errors)\n",
    "\n",
    "    return obs_abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_IMF_obs = []\n",
    "log10_N_Ia_obs = []\n",
    "simulations = 1000\n",
    "\n",
    "start = t.time()\n",
    "for i in tqdm(range(len(abundances))):\n",
    "    x = add_noise(abundances[i].detach().numpy())\n",
    "    alpha, N_Ia = posterior.sample((simulations,), x=x, show_progress_bars=False)[:,0:2].T\n",
    "    alpha_IMF_obs.append(alpha)\n",
    "    log10_N_Ia_obs.append(N_Ia)\n",
    "end = t.time()\n",
    "print(f'Time to run {simulations} simulations for {N_stars} stars: {end-start:.3f} s')\n",
    "\n",
    "alpha_IMF_obs = np.array(alpha_IMF_obs).ravel()\n",
    "log10_N_Ia_obs = np.array(log10_N_Ia_obs).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = np.arange(1,1000)\n",
    "n_stars_plot(alpha_IMF_obs, log10_N_Ia_obs, global_params[0], stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_hist_sides(alpha_IMF_obs, log10_N_Ia_obs, global_params[0], N_stars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_chempy_multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
