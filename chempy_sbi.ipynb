{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T11:18:21.960991Z",
     "start_time": "2024-08-10T11:18:17.508625Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguenes/miniconda3/envs/master_chempy_multi/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from Chempy.parameter import ModelParameters\n",
    "\n",
    "import sbi.utils as utils\n",
    "from sbi.inference.base import infer\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "#import ili\n",
    "\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time as t\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bae8e580878ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Neural Network to simulate Chempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d556874e9ccdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T11:18:25.478922Z",
     "start_time": "2024-08-10T11:18:25.345784Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------ Load & prepare the data ------\n",
    "\n",
    "# --- Load in training data ---\n",
    "path_training = '../ChempyMulti/tutorial_data/TNG_Training_Data.npz'\n",
    "training_data = np.load(path_training, mmap_mode='r')\n",
    "\n",
    "elements = training_data['elements']\n",
    "train_x = training_data['params']\n",
    "train_y = training_data['abundances']\n",
    "\n",
    "\n",
    "# ---  Load in the validation data ---\n",
    "path_test = '../ChempyMulti/tutorial_data/TNG_Test_Data.npz'\n",
    "val_data = np.load(path_test, mmap_mode='r')\n",
    "\n",
    "val_x = val_data['params']\n",
    "val_y = val_data['abundances']\n",
    "\n",
    "\n",
    "# --- Clean the data ---\n",
    "def clean_data(x, y):\n",
    "    # Remove all zeros from the training data\n",
    "    index = np.where((y == 0).all(axis=1))[0]\n",
    "    x = np.delete(x, index, axis=0)\n",
    "    y = np.delete(y, index, axis=0)\n",
    "\n",
    "    # Remove all infinite values from the training data\n",
    "    index = np.where(np.isfinite(y).all(axis=1))[0]\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_x, train_y = clean_data(train_x, train_y)\n",
    "val_x, val_y     = clean_data(val_x, val_y)\n",
    "\n",
    "\n",
    "# --- Normalize the data ---\n",
    "x_mean, x_std = train_x.mean(axis=0), train_x.std(axis=0)\n",
    "y_mean, y_std = train_y.mean(axis=0), train_y.std(axis=0)\n",
    "\n",
    "\n",
    "def normalize_data(x, y, x_mean=x_mean, x_std=x_std, y_mean=y_mean, y_std=y_std):\n",
    "    x = (x - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_x, train_y = normalize_data(train_x, train_y)\n",
    "val_x, val_y     = normalize_data(val_x, val_y)\n",
    "\n",
    "\n",
    "# add time squared as parameter\n",
    "def add_time_squared(x):\n",
    "    time_squared = np.array([x.T[-1]**2]).T\n",
    "    if len(x.shape) == 1:\n",
    "        return np.concatenate((x, time_squared))\n",
    "    elif len(x.shape) == 2:\n",
    "        return np.concatenate((x, time_squared), axis=1)\n",
    "\n",
    "\n",
    "train_x = add_time_squared(train_x)\n",
    "val_x = add_time_squared(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bba6990ccab0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T23:22:55.039292Z",
     "start_time": "2024-08-07T23:22:55.036065Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    model = MLPRegressor(solver=\\'adam\\', alpha=0.001, max_iter=epochs, learning_rate=\\'adaptive\\', tol=1e-13,\\n                         hidden_layer_sizes=(neurons,), activation=\\'tanh\\', verbose=verbose,\\n                         shuffle=True, early_stopping=True)\\n\\n    model.fit(x, y)\\n\\n    model_pred = model.predict(x)\\n    score = np.mean((model_pred-y)**2.)\\n    diff = np.abs(y-model_pred)\\n\\n    w0, w1 = model.coefs_\\n    b0, b1 = model.intercepts_\\n\\n    return score, diff, [w0, w1, b0, b1]\\n\\n\\n# --- Train the neural network ---\\n# Train an independent neural network for each element and save the weights\\noutput = []\\nneurons = 40\\nfor el_i, el in enumerate(elements):\\n    print(\"Running net %d of %d\" % (el_i + 1, len(elements)))\\n    o = single_regressor(train_x, train_y[:, el_i], neurons=neurons, epochs=3000, verbose=False)\\n    print(\"Score for element %s is %.3f\" % (el, o[0]))\\n    output.append(o)\\n\\n\\n# --- Save the neural network outputs ---\\nscores = [score for score, _, _ in output]\\ndiffs = [diff for _, diff, _ in output]\\ncoeffs = [co for _, _, co in output]\\n\\nw0 = np.hstack([co[0] for co in coeffs])\\nb0 = np.hstack([co[2] for co in coeffs])\\nb1 = np.hstack([co[3] for co in coeffs])\\n\\n# Read in w1 vector into sparse structure\\nw1 = np.zeros([w0.shape[1], b1.shape[0]])\\nassert neurons == w0.shape[1] / len(coeffs)\\nfor i in range(len(coeffs)):\\n    w1[int(neurons * i):int(neurons * (i + 1)), i] = coeffs[i][1][:, 0]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# ----- Train the neural network -----\n",
    "\n",
    "# --- Define the neural network ---\n",
    "def single_regressor(x, y, neurons=40, epochs=3000, verbose=False):\n",
    "    \"\"\"#Return out-of-sample score for a given number of neurons for one element\n",
    "\"\"\"\n",
    "    model = MLPRegressor(solver='adam', alpha=0.001, max_iter=epochs, learning_rate='adaptive', tol=1e-13,\n",
    "                         hidden_layer_sizes=(neurons,), activation='tanh', verbose=verbose,\n",
    "                         shuffle=True, early_stopping=True)\n",
    "\n",
    "    model.fit(x, y)\n",
    "\n",
    "    model_pred = model.predict(x)\n",
    "    score = np.mean((model_pred-y)**2.)\n",
    "    diff = np.abs(y-model_pred)\n",
    "\n",
    "    w0, w1 = model.coefs_\n",
    "    b0, b1 = model.intercepts_\n",
    "\n",
    "    return score, diff, [w0, w1, b0, b1]\n",
    "\n",
    "\n",
    "# --- Train the neural network ---\n",
    "# Train an independent neural network for each element and save the weights\n",
    "output = []\n",
    "neurons = 40\n",
    "for el_i, el in enumerate(elements):\n",
    "    print(\"Running net %d of %d\" % (el_i + 1, len(elements)))\n",
    "    o = single_regressor(train_x, train_y[:, el_i], neurons=neurons, epochs=3000, verbose=False)\n",
    "    print(\"Score for element %s is %.3f\" % (el, o[0]))\n",
    "    output.append(o)\n",
    "\n",
    "\n",
    "# --- Save the neural network outputs ---\n",
    "scores = [score for score, _, _ in output]\n",
    "diffs = [diff for _, diff, _ in output]\n",
    "coeffs = [co for _, _, co in output]\n",
    "\n",
    "w0 = np.hstack([co[0] for co in coeffs])\n",
    "b0 = np.hstack([co[2] for co in coeffs])\n",
    "b1 = np.hstack([co[3] for co in coeffs])\n",
    "\n",
    "# Read in w1 vector into sparse structure\n",
    "w1 = np.zeros([w0.shape[1], b1.shape[0]])\n",
    "assert neurons == w0.shape[1] / len(coeffs)\n",
    "for i in range(len(coeffs)):\n",
    "    w1[int(neurons * i):int(neurons * (i + 1)), i] = coeffs[i][1][:, 0]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363d8b3fe9be7467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T23:22:55.043596Z",
     "start_time": "2024-08-07T23:22:55.038447Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# --- Save the weights and normalization parameters ---\\n# Save output\\nnp.savez('data/tutorial_weights.npz',\\n         w0=w0, w1=w1, b0=b0, b1=b1,\\n         in_mean=x_mean, in_std=x_std, out_mean=y_mean, out_std=y_std,\\n         activation='tanh', neurons=neurons)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# --- Save the weights and normalization parameters ---\n",
    "# Save output\n",
    "np.savez('data/tutorial_weights.npz',\n",
    "         w0=w0, w1=w1, b0=b0, b1=b1,\n",
    "         in_mean=x_mean, in_std=x_std, out_mean=y_mean, out_std=y_std,\n",
    "         activation='tanh', neurons=neurons)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bc15ba2b614e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T23:27:12.892212Z",
     "start_time": "2024-08-07T23:26:03.673969Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 253us/step - loss: 0.0313 - val_loss: 0.0029\n",
      "Epoch 2/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 3/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 4/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 5/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 6/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250us/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 7/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 8/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254us/step - loss: 9.7501e-04 - val_loss: 8.3453e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1ms/step - loss: 8.7161e-04 - val_loss: 7.2441e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250us/step - loss: 8.3379e-04 - val_loss: 7.4768e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249us/step - loss: 8.0908e-04 - val_loss: 7.0793e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 248us/step - loss: 7.9724e-04 - val_loss: 9.1531e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249us/step - loss: 7.3128e-04 - val_loss: 9.1951e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 248us/step - loss: 7.5232e-04 - val_loss: 7.9866e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m14328/14328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250us/step - loss: 7.3091e-04 - val_loss: 7.5356e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x329426cf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- tensorflow NN -----\n",
    "# --- Define the neural network ---\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(100, activation='tanh'),\n",
    "        tf.keras.layers.Dense(40, activation='tanh'),\n",
    "        tf.keras.layers.Dense(train_y.shape[1])\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(train_x, train_y, validation_data=(val_x,val_y), epochs=15, batch_size=32)\n",
    "\n",
    "#model.save('data/tutorial_weights_tf.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d86028",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-07T23:22:55.046254Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Define the neural network ---\n",
    "#print(\".\")\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(train_x.shape[1], 100),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(100, 40),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(40, train_y.shape[1])\n",
    ")\n",
    "#print(\".\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#print(\".\")\n",
    "# --- Train the neural network ---\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    print(\".\")\n",
    "    for i in range(0, train_x.shape[0], batch_size):\n",
    "        x_batch = torch.tensor(train_x[i:i+batch_size], dtype=torch.float32)\n",
    "        y_batch = torch.tensor(train_y[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "#torch.save(model, 'data/tutorial_weights_pytorch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea71c69fff65021",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bf4e920a295be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T11:28:14.917963Z",
     "start_time": "2024-08-10T11:28:14.889713Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model_Torch(\n",
       "  (l1): Linear(in_features=7, out_features=100, bias=True)\n",
       "  (l2): Linear(in_features=100, out_features=40, bias=True)\n",
       "  (l3): Linear(in_features=40, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Load the Network -----\n",
    "# Load network weights trained in train_chempyNN.py\n",
    "x = np.load('data/tutorial_weights.npz')\n",
    "\n",
    "w0 = x['w0']\n",
    "w1 = x['w1']\n",
    "b0 = x['b0']\n",
    "b1 = x['b1']\n",
    "in_mean = x['in_mean']\n",
    "in_std = x['in_std']\n",
    "out_mean = x['out_mean']\n",
    "out_std = x['out_std']\n",
    "activation = x['activation']\n",
    "neurons = x['neurons']\n",
    "\n",
    "# --- Load the tensorflow model ---\n",
    "#model = tf.keras.models.load_model('data/tutorial_weights_tf.keras')\n",
    "\n",
    "# --- Load the pytorch model ---\n",
    "class Model_Torch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Torch, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(train_x.shape[1], 100)\n",
    "        self.l2 = torch.nn.Linear(100, 40)\n",
    "        self.l3 = torch.nn.Linear(40, train_y.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.l1(x))\n",
    "        x = torch.tanh(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_t = Model_Torch()\n",
    "model_t.load_state_dict(torch.load('data/pytorch_state_dict.pt'))\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"using mps\")\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model_t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ef8f7e6f19a250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T11:29:20.689710Z",
     "start_time": "2024-08-10T11:29:20.683238Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Set-up the Simulator -----\n",
    "def add_time_squared(x):\n",
    "    time_squared = np.array([x.T[-1]**2]).T\n",
    "    if len(x.shape) == 1:\n",
    "        return np.concatenate((x, time_squared))\n",
    "    elif len(x.shape) == 2:\n",
    "        return np.concatenate((x, time_squared), axis=1)\n",
    "    \n",
    "def simulator_NN_torch(in_par):\n",
    "    in_par = (in_par - in_mean) / in_std\n",
    "    in_par = add_time_squared(in_par)\n",
    "\n",
    "    in_par = torch.tensor(in_par, dtype=torch.float32).to(device)\n",
    "\n",
    "    out = model_t(in_par)\n",
    "\n",
    "    return out.detach().cpu().numpy() * out_std + out_mean\n",
    "    \n",
    "def simulator_NN_tf(in_par):\n",
    "    in_par = (in_par - in_mean) / in_std\n",
    "    in_par = add_time_squared(in_par)\n",
    "\n",
    "    out = model.predict(in_par, verbose=0)\n",
    "\n",
    "    return out * out_std + out_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8894d56d2b47cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T23:27:31.944444Z",
     "start_time": "2024-08-07T23:27:31.937621Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Set-up the Simulator -----\n",
    "def add_time_squared(x):\n",
    "    time_squared = np.array([x.T[-1]**2]).T\n",
    "    if len(x.shape) == 1:\n",
    "        return np.concatenate((x, time_squared))\n",
    "    elif len(x.shape) == 2:\n",
    "        return np.concatenate((x, time_squared), axis=1)\n",
    "\n",
    "\n",
    "def simulator_NN(in_par):\n",
    "    in_par = (in_par - in_mean) / in_std\n",
    "    in_par = add_time_squared(in_par)\n",
    "\n",
    "    l1 = np.matmul(in_par, w0) + b0\n",
    "    l2 = np.matmul(np.tanh(l1), w1) + b1\n",
    "\n",
    "    return l2 * out_std + out_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666980b6d732cffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T11:29:30.894907Z",
     "start_time": "2024-08-10T11:29:30.885361Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Set-up priors -----\n",
    "a = ModelParameters()\n",
    "priors = torch.tensor([[a.priors[opt][0], a.priors[opt][1]] for opt in a.to_optimize])\n",
    "\n",
    "combined_priors = utils.MultipleIndependent(\n",
    "    [Normal(p[0]*torch.ones(1), p[1]*torch.ones(1)) for p in priors] +\n",
    "    [Uniform(torch.tensor([2.0]), torch.tensor([12.8]))],\n",
    "    validate_args=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35326b77c6c42785",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-10T11:29:36.103970Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 1000 simulations.: 100%|██████████| 1000/1000 [00:00<00:00, 1627.35it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ----- sbi setup -----\n",
    "num_sim = 1000\n",
    "method = 'SNPE' #SNPE or SNLE or SNRE\n",
    "\n",
    "start = t.time()\n",
    "posterior = infer(\n",
    "    simulator_NN_torch,\n",
    "    combined_priors,\n",
    "    method=method,\n",
    "    num_simulations=num_sim)\n",
    "\n",
    "print(f'Time taken to train the posterior with {num_sim} samples: {round(t.time() - start, 4)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fd83b26f1193a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Save the posterior -----\n",
    "with open(\"data/posterior_SNPE_torch.pickle\", \"wb\") as f:\n",
    "    pickle.dump(posterior, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbf10c7fc14c98",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train ili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa83bb8ed92521e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader = ili.dataloaders.NumpyLoader(train_x, train_y)             # Create a data loader\n",
    "\n",
    "trainer = ili.inference.InferenceRunner.load(\n",
    "    backend = 'sbi',\n",
    "    engine='NPE',                # Choose a backend and inference engine (here, Neural Posterior Estimation)\n",
    "    prior = combined_priors,    # Define a prior\n",
    "    # Define a neural network architecture (here, MAF)\n",
    "    nets = [ili.utils.load_nde_sbi(engine='NPE', model='maf')]\n",
    ")\n",
    "\n",
    "start = t.time()\n",
    "posterior_ili, _ = trainer(loader)                  # Run training to map data -> parameters\n",
    "print(f'Time taken to train the posterior: {round(t.time() - start, 4)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dbc5cb08ad134",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = posterior.sample(                     # Generate 1000 samples from the posterior for input x[0]\n",
    "  x=train_x[0], sample_shape=(10,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cac18da3e8f51a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluate the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727b6d78619a83f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import pickle\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "from Chempy.parameter import ModelParameters\n",
    "import sbi.utils as utils\n",
    "\n",
    "\n",
    "# ------ Load & prepare the data ------\n",
    "\n",
    "# --- Load in training data ---\n",
    "path_training = '../ChempyMulti/tutorial_data/TNG_Training_Data.npz'\n",
    "training_data = np.load(path_training, mmap_mode='r')\n",
    "\n",
    "elements = training_data['elements']\n",
    "train_x = training_data['params']\n",
    "train_y = training_data['abundances']\n",
    "\n",
    "\n",
    "# ---  Load in the validation data ---\n",
    "path_test = '../ChempyMulti/tutorial_data/TNG_Test_Data.npz'\n",
    "val_data = np.load(path_test, mmap_mode='r')\n",
    "\n",
    "val_x = val_data['params']\n",
    "val_y = val_data['abundances']\n",
    "\n",
    "\n",
    "# --- Clean the data ---\n",
    "def clean_data(x, y):\n",
    "    # Remove all zeros from the training data\n",
    "    index = np.where((y == 0).all(axis=1))[0]\n",
    "    x = np.delete(x, index, axis=0)\n",
    "    y = np.delete(y, index, axis=0)\n",
    "\n",
    "    # Remove all infinite values from the training data\n",
    "    index = np.where(np.isfinite(y).all(axis=1))[0]\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_x, train_y = clean_data(train_x, train_y)\n",
    "val_x, val_y     = clean_data(val_x, val_y)\n",
    "\n",
    "\n",
    "# --- Normalize the data ---\n",
    "x_mean, x_std = train_x.mean(axis=0), train_x.std(axis=0)\n",
    "y_mean, y_std = train_y.mean(axis=0), train_y.std(axis=0)\n",
    "\n",
    "\n",
    "def normalize_data(x, y, x_mean=x_mean, x_std=x_std, y_mean=y_mean, y_std=y_std):\n",
    "    x = (x - x_mean) / x_std\n",
    "    y = (y - y_mean) / y_std\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_x, train_y = normalize_data(train_x, train_y)\n",
    "val_x, val_y     = normalize_data(val_x, val_y)\n",
    "\n",
    "\n",
    "# add time squared as parameter\n",
    "def add_time_squared(x):\n",
    "    time_squared = np.array([x.T[-1]**2]).T\n",
    "    if len(x.shape) == 1:\n",
    "        return np.concatenate((x, time_squared))\n",
    "    elif len(x.shape) == 2:\n",
    "        return np.concatenate((x, time_squared), axis=1)\n",
    "\n",
    "\n",
    "train_x = add_time_squared(train_x)\n",
    "val_x = add_time_squared(val_x)\n",
    "\n",
    "\n",
    "# ----- Load the Network -----\n",
    "# Load network weights trained in train_chempyNN.py\n",
    "x = np.load('data/tutorial_weights.npz')\n",
    "\n",
    "w0 = x['w0']\n",
    "w1 = x['w1']\n",
    "b0 = x['b0']\n",
    "b1 = x['b1']\n",
    "in_mean = x['in_mean']\n",
    "in_std = x['in_std']\n",
    "out_mean = x['out_mean']\n",
    "out_std = x['out_std']\n",
    "activation = x['activation']\n",
    "neurons = x['neurons']\n",
    "\n",
    "\n",
    "# ----- Set-up the Simulator -----\n",
    "def simulator_NN(in_par):\n",
    "    in_par = (in_par - in_mean) / in_std\n",
    "    in_par = add_time_squared(in_par)\n",
    "\n",
    "    l1 = np.matmul(in_par, w0) + b0\n",
    "    l2 = np.matmul(np.tanh(l1), w1) + b1\n",
    "\n",
    "    return l2 * out_std + out_mean\n",
    "\n",
    "\n",
    "# ----- Set-up priors -----\n",
    "a = ModelParameters()\n",
    "labels=[a.to_optimize[i] for i in range(len(a.to_optimize))] + ['time']\n",
    "priors = torch.tensor([[a.priors[opt][0], a.priors[opt][1]] for opt in a.to_optimize])\n",
    "\n",
    "combined_priors = utils.MultipleIndependent(\n",
    "    [Normal(p[0]*torch.ones(1), p[1]*torch.ones(1)) for p in priors] +\n",
    "    [Uniform(torch.tensor([2.0]), torch.tensor([12.8]))],\n",
    "    validate_args=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac8f3634e06ac7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Load the posterior -----\n",
    "with open(\"data/posterior_SNPE.pickle\", \"rb\") as f:\n",
    "    posterior = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20930971dda1a3ca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Evaluate the posterior -----\n",
    "# ----- Simulate data -----\n",
    "prior = combined_priors.sample((1,))\n",
    "simulated_data = simulator_NN(prior)\n",
    "\n",
    "posterior_samples = posterior.sample((10000,), x=simulated_data)\n",
    "_ = pairplot(posterior_samples, figsize=(15, 15), points=prior, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eceeb7ac451fca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----- Evaluate the posterior -----\n",
    "# evaluation data\n",
    "index = 100\n",
    "x = val_x[index][:-1]*x_std + x_mean\n",
    "y = val_y[index]\n",
    "\n",
    "posterior_samples = posterior.sample((10000,), x=y)\n",
    "_ = pairplot(posterior_samples, figsize=(15, 15), points=x, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfad49fe11485ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
